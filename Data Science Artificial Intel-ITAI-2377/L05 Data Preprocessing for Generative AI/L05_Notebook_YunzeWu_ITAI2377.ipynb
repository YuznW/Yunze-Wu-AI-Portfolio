{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVFTLDQxX9Lj"
      },
      "source": [
        "#Title and Introduction\n",
        "##Data Preprocessing Lab: Generative AI\n",
        "###    Welcome to the Data Preprocessing Lab for Generative AI!\n",
        "   In this lab, you'll get hands-on experience with key preprocessing techniques for both text and (optionally) image data.\n",
        "\n",
        "   **Learning Objectives:**\n",
        "\n",
        "\n",
        "*  Understand and apply core data preprocessing techniques.\n",
        "*  Explore word embedding techniques (Word2Vec/GloVe, BERT).\n",
        "*  Analyze the impact of preprocessing choices on data quality and\n",
        "   model suitability. List item\n",
        "*   Practice using cosine similarity for comparing embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Zw4alNavxp"
      },
      "source": [
        "## Part 1: Environment Setup\n",
        "\n",
        "First, we'll install and import all necessary libraries. Run the following cell to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EqJsAv1W_DC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195191a1-081f-424f-c04f-7101c2519082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Setup complete! All required libraries have been imported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 1: Environment Setup\n",
        "#############################\n",
        "# This cell installs and imports all necessary libraries for our text preprocessing pipeline.\n",
        "# We'll be using:\n",
        "# - pandas & numpy: for data manipulation\n",
        "# - nltk: for natural language processing tasks\n",
        "# - scikit-learn: for machine learning utilities\n",
        "# - transformers & torch: for BERT embeddings\n",
        "# - gensim: for word embeddings (Word2Vec/GloVe)\n",
        "\n",
        "# Install required packages\n",
        "%pip install pandas numpy nltk scikit-learn transformers torch datasets gensim\n",
        "\n",
        "# TODO: Import the required libraries\n",
        "# Hint: You need pandas, numpy, nltk, and sklearn components\n",
        "# YOUR CODE HERE - import the basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "# Add more imports as needed...\n",
        "\n",
        "# These are more advanced imports you'll need later\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Download required NLTK data\n",
        "# These are necessary for tokenization, stop words, and lemmatization\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"Setup complete! All required libraries have been imported.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLnCpVjOeb8m"
      },
      "source": [
        "## Part 2: Loading and Exploring the BBC News Dataset\n",
        "\n",
        "We'll now load the BBC News dataset  you used in previous assignments, and perform initial exploration of its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mty-ZhRFeuxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "8055fedb-75f2-4752-ed99-35c8465d5803"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 30 fields in line 7, saw 34\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2205828545.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbc-news-data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# TODO: Rename the columns to match our processing pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 30 fields in line 7, saw 34\n"
          ]
        }
      ],
      "source": [
        "# SECTION 2: Data Loading and Initial Exploration\n",
        "##############################################\n",
        "# Here we load the BBC News dataset and perform initial analysis\n",
        "# Understanding our data is crucial before applying any preprocessing\n",
        "\n",
        "# Load the dataset\n",
        "# SECTION 2: Data Loading and Initial Exploration\n",
        "##############################################\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('bbc-news-data.csv', encoding='latin1')\n",
        "\n",
        "# TODO: Rename the columns to match our processing pipeline\n",
        "# Hint: The original columns are 'Text' and 'Category'\n",
        "# YOUR CODE HERE\n",
        "df = df.rename(columns={'Text':'text', 'Category': 'category'})\n",
        "\n",
        "# TODO: Perform basic data exploration\n",
        "# TASK 1: Display the first few rows and basic information about the dataset\n",
        "# Hint: Use pandas' head(), info(), and describe() methods\n",
        "# YOUR CODE HERE\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe()\n",
        "\n",
        "# TASK 2: Analyze the distribution of categories\n",
        "# Hint: Use value_counts() on the category column\n",
        "# YOUR CODE HERE\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# TASK 3: Calculate and display basic text statistics\n",
        "# Calculate average text length per category\n",
        "df['text_length'] = df['text'].str.len()\n",
        "\n",
        "# TODO: Create a visualization of text lengths by category\n",
        "# Hint: Use seaborn's boxplot\n",
        "# YOUR CODE HERE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a boxplot to visualize text lengths by category\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='category', y='text_length', data=df)\n",
        "plt.title('Text Lengths by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Text Length')\n",
        "\n",
        "# Display your findings\n",
        "print(\"\\nDataset Statistics:\")\n",
        "# TODO: Add code to display your findings\n",
        "# YOUR CODE HERE\n",
        "print(\"\\nðŸ“Š Average Text Length per Category:\")\n",
        "print(df.groupby('category')['text_length'].mean())\n",
        "\n",
        "print(\"\\nâœ… Dataset exploration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZS4FAngetyg"
      },
      "source": [
        "# Comprehension Questions - Data Exploration\n",
        "\n",
        "Answer the following questions based on the dataset exploration above:\n",
        "\n",
        "1. What are the dimensions of our dataset?\n",
        "2. How many different categories are there in the news articles?\n",
        "3. Is the dataset balanced across categories? Why might this matter?\n",
        "4. Are there any missing values that need to be addressed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol54jBX4fKTX"
      },
      "source": [
        "## Part 3: Text Preprocessing\n",
        "\n",
        "We'll now implement basic text preprocessing steps to clean our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocseDUoofFyA"
      },
      "outputs": [],
      "source": [
        "# SECTION 3: Text Cleaning and Preprocessing\n",
        "########################################\n",
        "# This section implements fundamental text preprocessing steps:\n",
        "# 1. Converting to lowercase (why? -> maintains consistency)\n",
        "# 2. Removing special characters (why? -> reduces noise)\n",
        "# 3. Handling whitespace (why? -> standardizes format)\n",
        "\n",
        "########################################\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Performs basic text cleaning operations.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text to be cleaned\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following steps:\n",
        "    # 1. Convert to lowercase\n",
        "    # 2. Remove URLs and emails\n",
        "    # 3. Remove special characters but keep sentence structure\n",
        "    # 4. Remove extra whitespace\n",
        "    # Hint: Use string methods and regular expressions\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    return text  # Replace with your cleaned text\n",
        "\n",
        "# Test the function with a sample\n",
        "sample_text = \"Hello, World! This is a TEST... 123\"\n",
        "print(\"Original:\", sample_text)\n",
        "print(\"Cleaned:\", clean_text(sample_text))\n",
        "\n",
        "# Apply to the entire dataset\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqTwxjjMfSUU"
      },
      "source": [
        "## Part 4: Tokenization and Advanced Processing\n",
        "\n",
        "Now we'll tokenize our text and apply more advanced preprocessing techniques including:\n",
        "- Tokenization\n",
        "- Stop word removal\n",
        "- Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inv4_vNXfYTw"
      },
      "outputs": [],
      "source": [
        "# SECTION 4: Tokenization and Advanced Processing\n",
        "#############################################\n",
        "# This section implements more sophisticated NLP techniques:\n",
        "# - Tokenization: splitting text into words\n",
        "# - Stop word removal: removing common words\n",
        "# - Lemmatization: reducing words to their base form\n",
        "# Check if you do not need to install any additional libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize our tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_and_process(text):\n",
        "    \"\"\"\n",
        "    Performs advanced text processing including tokenization,\n",
        "    stop word removal, and lemmatization.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Cleaned text to process\n",
        "\n",
        "    Returns:\n",
        "    list: List of processed tokens\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following steps:\n",
        "    # 1. Tokenize the text\n",
        "    # 2. Remove stop words\n",
        "    # 3. Apply lemmatization\n",
        "    # Hint: Use the initialized stop_words and lemmatizer\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    tokens = []  # Replace with actual tokenization\n",
        "    processed_tokens = []  # Replace with processed tokens\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "# Test the function\n",
        "sample_text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "processed_result = tokenize_and_process(sample_text)\n",
        "print(\"Original:\", sample_text)\n",
        "print(\"Processed:\", processed_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbGlol8gAOP"
      },
      "source": [
        "## Part 5: Word Embeddings with GloVe\n",
        "\n",
        "We'll now generate word embeddings using pre-trained GloVe vectors. These embeddings will help us capture semantic relationships between words in our articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSSYtJsMgQ13"
      },
      "outputs": [],
      "source": [
        "# SECTION 5: Word Embeddings with GloVe\n",
        "###################################\n",
        "# This section generates word embeddings using pre-trained GloVe vectors\n",
        "# Word embeddings capture semantic relationships between words\n",
        "# by representing them as dense vectors in a high-dimensional space\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# SECTION 5: Word Embeddings with GloVe\n",
        "###################################\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def get_word2vec_embedding(text, model):\n",
        "    \"\"\"\n",
        "    Generates document embeddings by averaging word vectors.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "    model: Pre-trained word embedding model\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: Document embedding vector\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following steps:\n",
        "    # 1. Tokenize the input text\n",
        "    # 2. Get embedding for each token\n",
        "    # 3. Average the embeddings\n",
        "    # Hint: Handle words not in vocabulary\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    embeddings = []  # Replace with actual embeddings\n",
        "\n",
        "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)\n",
        "\n",
        "# Apply to a sample of the dataset\n",
        "sample_size = 100\n",
        "sample_df = df.head(sample_size).copy()\n",
        "sample_df['glove_embedding'] = sample_df['cleaned_text'].apply(\n",
        "    lambda x: get_word2vec_embedding(x, glove_model)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpMEHpOBglZM"
      },
      "source": [
        "## Part 6: BERT Embeddings\n",
        "\n",
        "Now we'll use BERT to generate contextual embeddings. BERT provides context-aware embeddings that can capture more nuanced relationships in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW26htSIgzrL"
      },
      "outputs": [],
      "source": [
        "# SECTION 6: BERT Embeddings\n",
        "#########################\n",
        "# This section implements BERT (Bidirectional Encoder Representations from Transformers)\n",
        "# BERT provides context-aware embeddings, meaning the same word can have different\n",
        "# embeddings based on its context in the sentence.\n",
        "# Key differences from GloVe:\n",
        "# - Contextual (words have different vectors based on context)\n",
        "# - Deep bidirectional (considers both left and right context)\n",
        "# - Pre-trained on massive datasets\n",
        "\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text, max_length=512):\n",
        "    \"\"\"\n",
        "    Generates BERT embeddings for a text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "    max_length (int): Maximum sequence length for BERT\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: BERT embedding vector\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following steps:\n",
        "    # 1. Tokenize the text using BERT tokenizer\n",
        "    # 2. Generate BERT embeddings\n",
        "    # 3. Extract the [CLS] token embedding\n",
        "    # Hint: Use tokenizer() and model() functions\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1: Tokenize\n",
        "    inputs = None  # Replace with actual tokenization\n",
        "\n",
        "    # Step 2: Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = None  # Replace with model output\n",
        "\n",
        "    # Step 3: Extract [CLS] token embedding\n",
        "    sentence_embedding = None  # Replace with correct embedding extraction\n",
        "\n",
        "    return sentence_embedding\n",
        "\n",
        "# Test the function\n",
        "test_text = \"This is a test sentence for BERT embeddings.\"\n",
        "bert_embedding = get_bert_embedding(test_text)\n",
        "print(\"BERT embedding shape:\", bert_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otNk1ANHg3AV"
      },
      "source": [
        "## Part 7: Comparing Embeddings\n",
        "\n",
        "Let's analyze how well our different embedding methods capture semantic relationships by comparing similarities between articles in the same and different categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cepwvccgg9NQ"
      },
      "outputs": [],
      "source": [
        "# SECTION 7: Similarity Analysis\n",
        "############################\n",
        "# This section implements methods to compare different embedding approaches\n",
        "# We'll analyze how well each embedding type captures semantic relationships\n",
        "# by comparing similarities between articles in the same and different categories\n",
        "# SECTION 6: BERT Embeddings\n",
        "#########################\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text, max_length=512):\n",
        "    \"\"\"\n",
        "    Generates BERT embeddings for a text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "    max_length (int): Maximum sequence length for BERT\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: BERT embedding vector\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following steps:\n",
        "    # 1. Tokenize the text using BERT tokenizer\n",
        "    # 2. Generate BERT embeddings\n",
        "    # 3. Extract the [CLS] token embedding\n",
        "    # Hint: Use tokenizer() and model() functions\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1: Tokenize\n",
        "    inputs = None  # Replace with actual tokenization\n",
        "\n",
        "    # Step 2: Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = None  # Replace with model output\n",
        "\n",
        "    # Step 3: Extract [CLS] token embedding\n",
        "    sentence_embedding = None  # Replace with correct embedding extraction\n",
        "\n",
        "    return sentence_embedding\n",
        "\n",
        "# Test the function\n",
        "test_text = \"This is a test sentence for BERT embeddings.\"\n",
        "bert_embedding = get_bert_embedding(test_text)\n",
        "print(\"BERT embedding shape:\", bert_embedding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPQN48cRAy6"
      },
      "source": [
        " ##SECTION 8: Detailed Similarity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Q2R1knhLS3"
      },
      "outputs": [],
      "source": [
        "# SECTION 8: Detailed Similarity Analysis\n",
        "####################################\n",
        "# This section analyzes how well our embeddings capture\n",
        "# semantic relationships between articles\n",
        "\n",
        "# SECTION 8: Detailed Similarity Analysis\n",
        "####################################\n",
        "\n",
        "def analyze_category_similarities(similarities, categories):\n",
        "    \"\"\"\n",
        "    Analyzes similarities within and across categories.\n",
        "\n",
        "    Parameters:\n",
        "    similarities (numpy.array): Similarity matrix\n",
        "    categories (list): List of category labels\n",
        "\n",
        "    Returns:\n",
        "    dict: Statistics about similarities\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following analysis:\n",
        "    # 1. Separate similarities into same-category and different-category groups\n",
        "    # 2. Calculate statistics for each group\n",
        "    # Hint: Use nested loops to compare categories\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    same_category_sims = []\n",
        "    diff_category_sims = []\n",
        "\n",
        "    # Your implementation here\n",
        "\n",
        "    return {\n",
        "        'same_category': {\n",
        "            'mean': np.mean(same_category_sims),\n",
        "            'std': np.std(same_category_sims)\n",
        "        },\n",
        "        'diff_category': {\n",
        "            'mean': np.mean(diff_category_sims),\n",
        "            'std': np.std(diff_category_sims)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Analyze both embedding types\n",
        "glove_analysis = analyze_category_similarities(glove_similarities, sample_df['category'].values)\n",
        "bert_analysis = analyze_category_similarities(bert_similarities, sample_df['category'].values)\n",
        "\n",
        "# Print results\n",
        "print(\"=== Similarity Analysis Results ===\")\n",
        "# TODO: Format and display the analysis results\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKF9qNrQtmU"
      },
      "source": [
        "## Part 9 Vizualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELmvCmhfPzNm"
      },
      "outputs": [],
      "source": [
        "# SECTION 9: Visualization of Results\n",
        "################################\n",
        "# This section creates visualizations to help us understand\n",
        "# the differences between our embedding approaches\n",
        "################################\n",
        "\n",
        "def plot_similarity_distributions(glove_sims, bert_sims, categories):\n",
        "    \"\"\"\n",
        "    Creates visualization comparing GloVe and BERT similarity distributions.\n",
        "\n",
        "    Parameters:\n",
        "    glove_sims (numpy.array): GloVe similarity matrix\n",
        "    bert_sims (numpy.array): BERT similarity matrix\n",
        "    categories (list): Category labels\n",
        "    \"\"\"\n",
        "    # TODO: Create the following visualizations:\n",
        "    # 1. Histogram or density plot of similarities\n",
        "    # 2. Box plot comparing same-category vs different-category similarities\n",
        "    # 3. Add appropriate labels and titles\n",
        "    # Hint: Use plt.subplots() for multiple plots\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Add your visualization code here\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "plot_similarity_distributions(glove_similarities,\n",
        "                            bert_similarities,\n",
        "                            sample_df['category'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YT3YMwpk1N"
      },
      "source": [
        "## Part 10 Statistical Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jdj8UFRpqcD"
      },
      "outputs": [],
      "source": [
        "# SECTION 10: Statistical Comparison\n",
        "###############################\n",
        "# This section performs statistical tests to compare\n",
        "# the effectiveness of different embedding approaches\n",
        "###############################\n",
        "\n",
        "def compare_embedding_methods(glove_analysis, bert_analysis):\n",
        "    \"\"\"\n",
        "    Performs statistical comparison of embedding methods.\n",
        "\n",
        "    Parameters:\n",
        "    glove_analysis (dict): GloVe similarity analysis results\n",
        "    bert_analysis (dict): BERT similarity analysis results\n",
        "    \"\"\"\n",
        "    # TODO: Implement the following analyses:\n",
        "    # 1. Calculate effect sizes for both methods\n",
        "    # 2. Perform statistical tests comparing the methods\n",
        "    # 3. Summarize the findings\n",
        "    # Hint: Consider using t-tests or Mann-Whitney U tests\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Print summary of findings\n",
        "    print(\"=== Statistical Comparison Results ===\")\n",
        "    # Add your summary here\n",
        "\n",
        "# Run comparison\n",
        "compare_embedding_methods(glove_analysis, bert_analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChIyR1yER1QT"
      },
      "source": [
        "## Part 11 Metrics and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G71L0hTpxQP"
      },
      "outputs": [],
      "source": [
        "# SECTION 11: Performance Evaluation\n",
        "##############################\n",
        "# This section calculates various metrics to evaluate\n",
        "# the quality of our embeddings\n",
        "\n",
        "##############################\n",
        "\n",
        "def calculate_metrics(similarities, categories):\n",
        "    \"\"\"\n",
        "    Calculates performance metrics for embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    similarities (numpy.array): Similarity matrix\n",
        "    categories (list): Category labels\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary of performance metrics\n",
        "    \"\"\"\n",
        "    # TODO: Implement various metrics such as:\n",
        "    # 1. Category separation score\n",
        "    # 2. Silhouette score\n",
        "    # 3. Custom metrics you design\n",
        "    # Hint: Consider what makes embeddings \"good\" for your use case\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    metrics = {}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for both embedding types\n",
        "glove_metrics = calculate_metrics(glove_similarities, sample_df['category'])\n",
        "bert_metrics = calculate_metrics(bert_similarities, sample_df['category'])\n",
        "\n",
        "# Display results\n",
        "print(\"=== Performance Metrics ===\")\n",
        "# TODO: Format and display the metrics\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpdQ3IRWhane"
      },
      "source": [
        "## Part 12: Final Analysis Questions\n",
        "\n",
        "1. Compare the similarity distributions for GloVe and BERT embeddings:\n",
        "   - Which method better distinguishes between same-category and different-category articles?\n",
        "   - What might explain the differences in performance?\n",
        "\n",
        "2. Based on the visualizations:\n",
        "   - What patterns do you notice in the similarity distributions?\n",
        "   - Are there any unexpected results?\n",
        "\n",
        "3. Considering the entire preprocessing pipeline:\n",
        "   - Which steps had the biggest impact on the final results?\n",
        "   - What additional preprocessing steps might improve the results?\n",
        "   - How would you modify this pipeline for different types of text data?\n",
        "\n",
        "4. Ethical Considerations:\n",
        "   - What biases might be present in our preprocessing pipeline?\n",
        "   - How might these biases affect the analysis of news articles?\n",
        "   - What steps could we take to mitigate these biases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QDG1lH6puq_"
      },
      "source": [
        "###Assessment Criteria:\n",
        "\n",
        "  * Correct implementation of cosine similarity\n",
        "  *Proper normalization of embeddings\n",
        "  *Effective visualization of results\n",
        "\n",
        "\n",
        "##Grading Rubric\n",
        "\n",
        "* Environment Setup: 10%\n",
        "* Data Exploration: 15%\n",
        "* Text Preprocessing: 20%\n",
        "* Word Embeddings Implementation: 25%\n",
        "* Similarity Analysis: 20%\n",
        "Final Analysis & Discussion: 10%\n",
        "\n",
        "##Common Issues and Solutions\n",
        "\n",
        "1. Memory Issues:\n",
        "\n",
        "* Implement batch processing for large datasets\n",
        "* Use appropriate data types (float32 vs float64)\n",
        "* Clear unused variables and call garbage collection\n",
        "\n",
        "\n",
        "2. Performance Optimization:\n",
        "\n",
        "* Vectorize operations where possible\n",
        "* Use appropriate batch sizes for BERT\n",
        "* Implement caching for embeddings\n",
        "\n",
        "\n",
        "3. Error Handling:\n",
        "\n",
        "* Implement robust error checking\n",
        "* Provide clear error messages\n",
        "* Handle edge cases appropriately"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}